"""
Cross-Lingual LongPPL Task Implementation.

Evaluates long-context capability on any language by transferring key paragraph
positions from English using cross-lingual embedding alignment.

Validated: Spearman rho=0.978 correlation with native evaluation across 12 languages.
"""

import json
import logging
import math
import os
from typing import Any, Dict, List, Literal, Optional, Tuple, Union

import numpy as np

from lm_eval.api.instance import Instance
from lm_eval.api.task import ConfigurableTask


eval_logger = logging.getLogger(__name__)

# Default HuggingFace dataset with pre-computed aligned docs
DEFAULT_HF_DATASET = "dzautner/crosslingual-longppl-eurovoc"

# Environment variable to override with local aligned docs
ALIGNED_DOCS_ENV_VAR = "CROSSLINGUAL_LONGPPL_ALIGNED_DOCS"

# Default configuration (validated settings)
DEFAULT_TOP_K = 8
DEFAULT_ALIGNMENT_THRESHOLD = 0.5

# Minimum paragraphs required per document
MIN_PARAGRAPHS = 3

# Maximum perplexity before filtering (matches validated methodology)
MAX_PPL_THRESHOLD = 100.0


class CrossLingualLongPPL(ConfigurableTask):
    """
    Cross-Lingual LongPPL Transfer Task.

    This task evaluates language model perplexity on key paragraphs that have been
    aligned from English using cross-lingual embeddings. It enables evaluation of
    long-context capability on any language without requiring the model to understand
    that language.

    The task uses pre-computed aligned documents (generated by prepare_aligned_docs.py)
    which contain:
    - Key paragraph indices identified by a teacher model on English text
    - Aligned paragraph indices in target languages via embedding similarity

    Configuration (via metadata):
        target_lang: Target language code (e.g., 'fi', 'de', 'lt')
        top_k: Number of key paragraphs per document (default: 8)
        aligned_docs_path: Path to pre-computed aligned documents JSON

    Environment:
        CROSSLINGUAL_LONGPPL_ALIGNED_DOCS: Path to aligned docs if not in config
    """

    VERSION = 1.0
    DATASET_PATH = None
    DATASET_NAME = None
    OUTPUT_TYPE = "loglikelihood_rolling"

    def __init__(
        self,
        data_dir: Optional[str] = None,
        cache_dir: Optional[str] = None,
        download_mode=None,
        config: Optional[dict] = None,
    ) -> None:
        super().__init__(
            data_dir=data_dir,
            cache_dir=cache_dir,
            download_mode=download_mode,
            config=config,
        )

        # Extract configuration from metadata
        metadata = self.config.metadata if self.config.metadata else {}

        self.target_lang: str = metadata.get("target_lang", "en")
        self.top_k: int = metadata.get("top_k", DEFAULT_TOP_K)

        # Get aligned docs path from metadata or environment
        self.aligned_docs_path: Optional[str] = metadata.get("aligned_docs_path")
        if not self.aligned_docs_path:
            self.aligned_docs_path = os.environ.get(ALIGNED_DOCS_ENV_VAR)

        self._aligned_docs: Optional[List[Dict]] = None
        self._doc_list: Optional[List[Dict]] = None

        eval_logger.info(
            f"CrossLingualLongPPL initialized: lang={self.target_lang}, top_k={self.top_k}"
        )

    def _load_aligned_docs(self) -> None:
        """Load pre-computed aligned documents lazily.

        Loading priority:
        1. Local file path from metadata (aligned_docs_path)
        2. Local file path from environment variable (CROSSLINGUAL_LONGPPL_ALIGNED_DOCS)
        3. HuggingFace dataset (LumiOpen/crosslingual-longppl-eurovoc)
        """
        if self._aligned_docs is not None:
            return

        # Try local file first (from metadata or env var)
        if self.aligned_docs_path and os.path.exists(self.aligned_docs_path):
            eval_logger.info(f"Loading aligned docs from local file: {self.aligned_docs_path}")
            with open(self.aligned_docs_path, encoding="utf-8") as f:
                data = json.load(f)
            self._aligned_docs = data.get("doc_data", [])
            return

        # Fall back to HuggingFace dataset
        try:
            from datasets import load_dataset

            eval_logger.info(f"Loading aligned docs from HuggingFace: {DEFAULT_HF_DATASET}")
            dataset = load_dataset(DEFAULT_HF_DATASET, split="train")

            # Convert HF dataset to our format
            self._aligned_docs = []
            for row in dataset:
                self._aligned_docs.append(json.loads(row["data"]))

        except Exception as e:
            raise ValueError(
                f"Could not load aligned docs. Either:\n"
                f"  1. Set {ALIGNED_DOCS_ENV_VAR} to path of aligned_docs.json, or\n"
                f"  2. Ensure {DEFAULT_HF_DATASET} is accessible on HuggingFace Hub\n"
                f"Error: {e}"
            )

        # Build document list with key paragraphs for target language
        self._doc_list = []
        for doc in self._aligned_docs:
            key_paras = self._get_key_paragraphs(doc)
            if len(key_paras) >= MIN_PARAGRAPHS:
                self._doc_list.append({
                    "doc_id": doc.get("celex_id", ""),
                    "key_paras": key_paras,
                    "n_paras": len(key_paras),
                })

        eval_logger.info(
            f"Loaded {len(self._doc_list)} documents for {self.target_lang}"
        )

    def _get_key_paragraphs(self, doc: Dict[str, Any]) -> List[str]:
        """Extract key paragraphs for target language from aligned document."""
        lang_data = doc.get("lang_data", {}).get(self.target_lang, {})
        key_indices = lang_data.get("key_indices", [])
        paragraphs = lang_data.get("paras", [])

        if not key_indices or not paragraphs:
            return []

        key_paras = []
        for idx in key_indices[: self.top_k]:
            if idx < len(paragraphs):
                para = paragraphs[idx]
                # Skip very short paragraphs (less than ~5 words)
                if len(para.split()) >= 5:
                    key_paras.append(para)

        return key_paras

    def has_training_docs(self) -> bool:
        return False

    def has_validation_docs(self) -> bool:
        return True

    def has_test_docs(self) -> bool:
        return False

    def training_docs(self):
        return None

    def validation_docs(self):
        """Return validation documents."""
        self._load_aligned_docs()
        return self._doc_list

    def test_docs(self):
        return None

    def fewshot_examples(self, k: int, rnd) -> List:
        """Perplexity tasks don't use few-shot examples."""
        if k != 0:
            raise ValueError(
                "The number of fewshot examples must be 0 for perplexity tasks."
            )
        return []

    def fewshot_context(self, doc: dict, num_fewshot: int) -> Literal[""]:
        """Perplexity tasks don't use few-shot context."""
        if num_fewshot != 0:
            raise ValueError(
                "The number of fewshot examples must be 0 for perplexity tasks."
            )
        return ""

    def doc_to_text(self, doc: Dict[str, Any]) -> str:
        """Return empty string - we use construct_requests for text."""
        return ""

    def doc_to_target(self, doc: Dict[str, Any]) -> str:
        """Return empty string for perplexity task."""
        return ""

    def construct_requests(
        self,
        doc: Dict[str, Any],
        ctx: str,
        **kwargs,
    ) -> List[Instance]:
        """
        Construct loglikelihood_rolling requests for each key paragraph.

        Each paragraph is evaluated independently, and results are averaged
        in process_results to produce the final perplexity.
        """
        if ctx:
            raise ValueError(
                "CrossLingualLongPPL does not support few-shot context."
            )

        key_paras = doc.get("key_paras", [])

        if not key_paras:
            return []

        requests = []
        for i, para in enumerate(key_paras):
            requests.append(
                Instance(
                    request_type=self.OUTPUT_TYPE,
                    doc=doc,
                    arguments=(para,),
                    idx=i,
                    **kwargs,
                )
            )

        return requests

    def process_results(
        self, doc: Dict[str, Any], results: List[Tuple[float]]
    ) -> Dict[str, float]:
        """
        Process log-likelihood results into perplexity metrics.

        Each result corresponds to one key paragraph. We compute per-paragraph
        perplexity and return the mean.

        Args:
            doc: The document being evaluated.
            results: List of (loglikelihood,) tuples from loglikelihood_rolling.

        Returns:
            Dictionary with longppl and log2_longppl metrics.
        """
        if not results:
            return {"longppl": float("nan"), "log2_longppl": float("nan")}

        para_ppls = []
        for result in results:
            # loglikelihood_rolling returns (loglikelihood,) tuple
            if isinstance(result, (tuple, list)):
                loglikelihood = result[0] if result else None
            else:
                loglikelihood = result

            if loglikelihood is None or loglikelihood == 0:
                continue

            # PPL = exp(-loglikelihood)
            # loglikelihood_rolling returns per-token log-likelihood
            ppl = math.exp(-loglikelihood)

            # Cap extreme values (matching validated methodology)
            if ppl < MAX_PPL_THRESHOLD:
                para_ppls.append(ppl)

        # Require minimum valid paragraphs
        if len(para_ppls) < MIN_PARAGRAPHS:
            return {"longppl": float("nan"), "log2_longppl": float("nan")}

        mean_ppl = float(np.mean(para_ppls))

        return {
            "longppl": mean_ppl,
            "log2_longppl": math.log2(mean_ppl) if mean_ppl > 0 else float("nan"),
        }

    def aggregation(self) -> Dict[str, Any]:
        """Define aggregation functions for metrics."""
        return {
            "longppl": _nanmean,
            "log2_longppl": _nanmean,
        }

    def higher_is_better(self) -> Dict[str, bool]:
        """Lower perplexity is better."""
        return {
            "longppl": False,
            "log2_longppl": False,
        }


def _nanmean(items: List[float]) -> float:
    """Compute mean ignoring NaN values."""
    valid = [x for x in items if x is not None and not math.isnan(x)]
    return float(np.mean(valid)) if valid else float("nan")
